{"cells":[{"cell_type":"markdown","metadata":{"id":"HM53Y4MD4xeV"},"source":["# MON NAIVE BAYES"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting spacy\n","  Downloading spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n","Requirement already satisfied: nltk in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n","Collecting wasabi<1.2.0,>=0.9.1\n","  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n","Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n","  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n","Requirement already satisfied: jinja2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n","Collecting preshed<3.1.0,>=3.0.2\n","  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n","Collecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n","Collecting smart-open<7.0.0,>=5.2.1\n","  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n","Requirement already satisfied: setuptools in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (57.4.0)\n","Collecting cymem<2.1.0,>=2.0.2\n","  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.0)\n","Collecting spacy-legacy<3.1.0,>=3.0.11\n","  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.27.1)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","Collecting typer<0.10.0,>=0.3.0\n","  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n","Collecting weasel<0.4.0,>=0.1.0\n","  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n","Collecting srsly<3.0.0,>=2.4.3\n","  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n","Collecting murmurhash<1.1.0,>=0.28.0\n","  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n","Requirement already satisfied: numpy>=1.19.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.2)\n","Collecting thinc<8.3.0,>=8.1.8\n","  Downloading thinc-8.2.1-cp310-cp310-win_amd64.whl (1.5 MB)\n","Requirement already satisfied: click in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.8.8)\n","Requirement already satisfied: joblib in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n","Collecting annotated-types>=0.4.0\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.14.5\n","  Downloading pydantic_core-2.14.5-cp310-none-win_amd64.whl (1.9 MB)\n","Collecting typing-extensions>=4.6.1\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n","Collecting blis<0.8.0,>=0.7.8\n","  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n","Collecting confection<1.0.0,>=0.0.1\n","  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n","Requirement already satisfied: colorama in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting cloudpathlib<0.17.0,>=0.7.0\n","  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n","Installing collected packages: typing-extensions, pydantic-core, colorama, catalogue, annotated-types, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.2.0\n","    Uninstalling typing-extensions-4.2.0:\n","      Successfully uninstalled typing-extensions-4.2.0\n","  Attempting uninstall: colorama\n","    Found existing installation: colorama 0.4.5\n","    Uninstalling colorama-0.4.5:\n","      Successfully uninstalled colorama-0.4.5\n","Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 typing-extensions-4.8.0 wasabi-1.1.2 weasel-0.3.4\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n","You should consider upgrading via the 'c:\\Users\\Monica Evelyn\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n","Requirement already satisfied: numpy>=1.19.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n","Requirement already satisfied: setuptools in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: jinja2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n","Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.9)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.5.18.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.12)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: colorama in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\monica evelyn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n","You should consider upgrading via the 'c:\\Users\\Monica Evelyn\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["%pip install spacy nltk\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":440,"status":"ok","timestamp":1701255616625,"user":{"displayName":"Christopher Julius","userId":"06119393692124690863"},"user_tz":-420},"id":"H_yd6tKsAcLP"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\CJ\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\CJ\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\CJ\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords, words\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","import spacy\n","import string\n","from spacy.lang.en import English\n","from spacy.tokens import Doc\n","from spacy.tokenizer import Tokenizer\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from nltk.stem import LancasterStemmer\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('words')"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1701255623095,"user":{"displayName":"Christopher Julius","userId":"06119393692124690863"},"user_tz":-420},"id":"nFRzv0aEArgJ","outputId":"ff47ebe5-b187-47cc-ff75-e43b31072710"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id   sentiment                                            content\n","0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n","1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696     sadness                Funeral ceremony...gloomy friday...\n","3  1956967789  enthusiasm               wants to hang out with friends SOON!\n","4  1956968416     neutral  @dannycastillo We want to trade with someone w..."]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('./tweet_emotions.csv')\n","df.head()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 15539 entries, 1 to 39999\n","Data columns (total 3 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   tweet_id   15539 non-null  int64 \n"," 1   sentiment  15539 non-null  object\n"," 2   content    15539 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 1001.6+ KB\n"]}],"source":["# Hapus data yg kosong\n","df.dropna(inplace = True)\n","\n","# Hapus data duplikat\n","df.drop_duplicates(inplace = True)\n","\n","for x in df.index:\n","  if df.loc[x, \"sentiment\"] == 'empty':\n","    df.drop(x,inplace=True)\n","\n","df.info()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1956968477</td>\n","      <td>worry</td>\n","      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id   sentiment                                            content\n","1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696     sadness                Funeral ceremony...gloomy friday...\n","3  1956967789  enthusiasm               wants to hang out with friends SOON!\n","4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n","5  1956968477       worry  Re-pinging @ghostridah14: why didn't you go to..."]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sadness : 5165\n","enthusiasm : 759\n","neutral : 8638\n","worry : 8459\n","surprise : 2187\n","love : 3842\n","fun : 1776\n","hate : 1323\n","happiness : 5209\n","boredom : 179\n","relief : 1526\n","anger : 110\n","['sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun' 'hate'\n"," 'happiness' 'boredom' 'relief' 'anger']\n"]}],"source":["# Itung jumlah per sentiment\n","for x in df['sentiment'].unique():\n","    print(x, \":\", df['sentiment'].value_counts()[x])\n","\n","print(df['sentiment'].unique())"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 16366 entries, 0 to 39999\n","Data columns (total 3 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   tweet_id   16366 non-null  int64 \n"," 1   sentiment  16366 non-null  object\n"," 2   content    16366 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 1.0+ MB\n"]}],"source":["# Hapus yg ratusan dan dibawah 3000 (disuruh leo)\n","for x in df.index:\n","  if (df.loc[x, \"sentiment\"] == 'anger' or df.loc[x, \"sentiment\"] == 'boredom' or df.loc[x, \"sentiment\"] == 'enthusiasm'\n","      or df.loc[x, \"sentiment\"] == 'fun' or df.loc[x, \"sentiment\"] == 'relief'\n","      or df.loc[x, \"sentiment\"] == 'surprise'\n","      or df.loc[x, \"sentiment\"] == 'worry' or df.loc[x, \"sentiment\"] == 'neutral'):\n","    df.drop(x,inplace=True)\n","\n","df.info()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6262.6\n"]}],"source":["mean = len(df)/len(df['sentiment'].unique())\n","print(mean)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 26742 entries, 1 to 39999\n","Data columns (total 3 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   tweet_id   26742 non-null  int64 \n"," 1   sentiment  26742 non-null  object\n"," 2   content    26742 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 835.7+ KB\n"]}],"source":["# Hapus baris sesuai jumlah mean\n","# mean = 3842\n","for x in df['sentiment'].unique():\n","    if df['sentiment'].value_counts()[x] > mean:\n","        dropped = int(df['sentiment'].value_counts()[x] - mean)\n","        df = df.drop(df[df['sentiment'] == x].tail(dropped).index)\n","\n","df.info()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sadness : 5165\n","love : 3842\n","hate : 1323\n","happiness : 5209\n","['sadness' 'love' 'hate' 'happiness']\n","[5209 5165 3842 1323]\n"]}],"source":["# Itung jumlah per sentiment\n","for x in df['sentiment'].unique():\n","    print(x, \":\", df['sentiment'].value_counts()[x])\n","\n","print(df['sentiment'].unique())\n","print(df['sentiment'].value_counts().unique())"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1956968477</td>\n","      <td>worry</td>\n","      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1956968487</td>\n","      <td>sadness</td>\n","      <td>I should be sleep, but im not! thinking about ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id sentiment                                            content\n","1  1956967666   sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696   sadness                Funeral ceremony...gloomy friday...\n","4  1956968416   neutral  @dannycastillo We want to trade with someone w...\n","5  1956968477     worry  Re-pinging @ghostridah14: why didn't you go to...\n","6  1956968487   sadness  I should be sleep, but im not! thinking about ..."]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1701254706181,"user":{"displayName":"Monica Evelyn","userId":"07312078870992308835"},"user_tz":-420},"id":"4qbRIIYLrpU4","outputId":"19dd676b-1967-4444-bd63-23998ae5d34f"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                             content  \\\n","1  Layin n bed with a headache  ughhhh...waitin o...   \n","2                Funeral ceremony...gloomy friday...   \n","6  I should be sleep, but im not! thinking about ...   \n","8            @charviray Charlene my love. I miss you   \n","9         @kelcouch I'm sorry  at least it's Friday?   \n","\n","                                     cleaned_content  \n","1             layin n bed headach ughhhh waitin call  \n","2                       funer ceremoni gloomi friday  \n","6  sleep im think old friend want marri damn amp ...  \n","8                                  charlen love miss  \n","9                                 sorri least friday  \n"]}],"source":["# Initialize the Lancaster Stemmer from NLTK\n","stemmer = PorterStemmer()\n","\n","# Set of valid words from NLTK's words corpus\n","# valid_words = set(words.words())\n","\n","# Function to clean text, perform stemming, and filter words not in the corpus\n","def clean_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Replace non-alphanumeric characters with a single space\n","    text = re.sub(r'[^a-zA-Z0-9@]', ' ', text)\n","    \n","    # remove @ mention \n","    # Split the text into words\n","    words = text.split()\n","\n","    # Filter out words that start with '@'\n","    filtered_words = [word for word in words if not word.startswith('@')]\n","\n","    # Join the filtered words back into a string\n","    text = ' '.join(filtered_words)\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Tokenize the text\n","    words = nltk.word_tokenize(text)\n","\n","    # Filter words not in the corpus\n","    # cleaned_words = [word for word in words if word in valid_words]\n","\n","    # Remove stopwords and perform stemming\n","    stemmed_words = [stemmer.stem(word) for word in words if word not in stopwords.words('english') and '@' not in word]\n","    \n","    cleaned_text = ' '.join(stemmed_words)\n","    \n","    return cleaned_text\n","# Apply the cleaning function to the 'content' column\n","df['cleaned_content'] = df['content'].apply(clean_text)\n","\n","# Display the cleaned dataset\n","print(df[['content', 'cleaned_content']].head())"]},{"cell_type":"markdown","metadata":{"id":"HSsgLbxMD0RY"},"source":["Data Preparation"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701255641965,"user":{"displayName":"Christopher Julius","userId":"06119393692124690863"},"user_tz":-420},"id":"Z2N-YOXchnz6"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n","[notice] To update, run: C:\\Users\\CJ\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: BeautifulSoup4 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.12.2)\n","Requirement already satisfied: scikit-learn in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.3.2)\n","Requirement already satisfied: textblob in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.17.1)\n","Collecting gensim\n","  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/ab/b0/d58dc405fd60ab546ca714321235dc2d455b2dc06bfb4fc1092940c749fc/gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata\n","  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n","Requirement already satisfied: soupsieve>1.2 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from BeautifulSoup4) (2.5)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.23.2)\n","Requirement already satisfied: scipy>=1.5.0 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: nltk>=3.1 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from textblob) (3.8.1)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (6.4.0)\n","Requirement already satisfied: click in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n","Requirement already satisfied: tqdm in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk>=3.1->textblob) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\cj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n","Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n","   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n","   ---------------------------------------- 0.1/24.0 MB 1.6 MB/s eta 0:00:15\n","   ---------------------------------------- 0.2/24.0 MB 2.4 MB/s eta 0:00:11\n","    --------------------------------------- 0.5/24.0 MB 3.5 MB/s eta 0:00:07\n","   - -------------------------------------- 0.9/24.0 MB 5.2 MB/s eta 0:00:05\n","   - -------------------------------------- 1.0/24.0 MB 5.5 MB/s eta 0:00:05\n","   - -------------------------------------- 1.0/24.0 MB 5.5 MB/s eta 0:00:05\n","   - -------------------------------------- 1.0/24.0 MB 5.5 MB/s eta 0:00:05\n","   - -------------------------------------- 1.0/24.0 MB 5.5 MB/s eta 0:00:05\n","   - -------------------------------------- 1.0/24.0 MB 5.5 MB/s eta 0:00:05\n","   -- ------------------------------------- 1.2/24.0 MB 2.7 MB/s eta 0:00:09\n","   -- ------------------------------------- 1.4/24.0 MB 2.9 MB/s eta 0:00:08\n","   --- ------------------------------------ 2.2/24.0 MB 3.9 MB/s eta 0:00:06\n","   ---- ----------------------------------- 2.7/24.0 MB 4.5 MB/s eta 0:00:05\n","   ---- ----------------------------------- 3.0/24.0 MB 4.7 MB/s eta 0:00:05\n","   ----- ---------------------------------- 3.2/24.0 MB 4.6 MB/s eta 0:00:05\n","   ----- ---------------------------------- 3.5/24.0 MB 4.9 MB/s eta 0:00:05\n","   ------ --------------------------------- 4.0/24.0 MB 5.1 MB/s eta 0:00:04\n","   ------- -------------------------------- 4.2/24.0 MB 5.3 MB/s eta 0:00:04\n","   ------- -------------------------------- 4.6/24.0 MB 5.3 MB/s eta 0:00:04\n","   ------- -------------------------------- 4.7/24.0 MB 5.2 MB/s eta 0:00:04\n","   ------- -------------------------------- 4.7/24.0 MB 5.2 MB/s eta 0:00:04\n","   ------- -------------------------------- 4.7/24.0 MB 5.2 MB/s eta 0:00:04\n","   -------- ------------------------------- 5.3/24.0 MB 5.0 MB/s eta 0:00:04\n","   --------- ------------------------------ 5.5/24.0 MB 5.0 MB/s eta 0:00:04\n","   --------- ------------------------------ 5.8/24.0 MB 5.1 MB/s eta 0:00:04\n","   ---------- ----------------------------- 6.1/24.0 MB 5.1 MB/s eta 0:00:04\n","   ---------- ----------------------------- 6.1/24.0 MB 5.0 MB/s eta 0:00:04\n","   ---------- ----------------------------- 6.3/24.0 MB 4.9 MB/s eta 0:00:04\n","   ----------- ---------------------------- 6.7/24.0 MB 5.0 MB/s eta 0:00:04\n","   ----------- ---------------------------- 7.0/24.0 MB 5.1 MB/s eta 0:00:04\n","   ------------ --------------------------- 7.3/24.0 MB 5.2 MB/s eta 0:00:04\n","   ------------ --------------------------- 7.5/24.0 MB 5.1 MB/s eta 0:00:04\n","   ------------ --------------------------- 7.6/24.0 MB 5.0 MB/s eta 0:00:04\n","   ------------ --------------------------- 7.8/24.0 MB 5.0 MB/s eta 0:00:04\n","   ------------- -------------------------- 7.9/24.0 MB 4.9 MB/s eta 0:00:04\n","   ------------- -------------------------- 8.0/24.0 MB 4.8 MB/s eta 0:00:04\n","   ------------- -------------------------- 8.4/24.0 MB 4.9 MB/s eta 0:00:04\n","   -------------- ------------------------- 8.8/24.0 MB 5.0 MB/s eta 0:00:04\n","   --------------- ------------------------ 9.0/24.0 MB 5.1 MB/s eta 0:00:03\n","   --------------- ------------------------ 9.5/24.0 MB 5.2 MB/s eta 0:00:03\n","   ---------------- ----------------------- 9.7/24.0 MB 5.1 MB/s eta 0:00:03\n","   ---------------- ----------------------- 10.0/24.0 MB 5.2 MB/s eta 0:00:03\n","   ---------------- ----------------------- 10.2/24.0 MB 5.1 MB/s eta 0:00:03\n","   ----------------- ---------------------- 10.4/24.0 MB 5.2 MB/s eta 0:00:03\n","   ----------------- ---------------------- 10.8/24.0 MB 5.3 MB/s eta 0:00:03\n","   ------------------ --------------------- 11.1/24.0 MB 5.3 MB/s eta 0:00:03\n","   ------------------- -------------------- 11.5/24.0 MB 6.0 MB/s eta 0:00:03\n","   ------------------- -------------------- 11.9/24.0 MB 6.2 MB/s eta 0:00:02\n","   -------------------- ------------------- 12.3/24.0 MB 6.0 MB/s eta 0:00:02\n","   --------------------- ------------------ 12.6/24.0 MB 5.9 MB/s eta 0:00:02\n","   --------------------- ------------------ 12.9/24.0 MB 5.8 MB/s eta 0:00:02\n","   ---------------------- ----------------- 13.3/24.0 MB 5.8 MB/s eta 0:00:02\n","   ---------------------- ----------------- 13.7/24.0 MB 6.0 MB/s eta 0:00:02\n","   ----------------------- ---------------- 14.1/24.0 MB 6.0 MB/s eta 0:00:02\n","   ------------------------ --------------- 14.5/24.0 MB 6.0 MB/s eta 0:00:02\n","   ------------------------ --------------- 14.9/24.0 MB 6.1 MB/s eta 0:00:02\n","   ------------------------- -------------- 15.3/24.0 MB 6.5 MB/s eta 0:00:02\n","   ------------------------- -------------- 15.4/24.0 MB 6.2 MB/s eta 0:00:02\n","   -------------------------- ------------- 15.9/24.0 MB 6.4 MB/s eta 0:00:02\n","   --------------------------- ------------ 16.3/24.0 MB 6.4 MB/s eta 0:00:02\n","   --------------------------- ------------ 16.6/24.0 MB 6.7 MB/s eta 0:00:02\n","   ---------------------------- ----------- 16.9/24.0 MB 6.7 MB/s eta 0:00:02\n","   ----------------------------- ---------- 17.5/24.0 MB 6.8 MB/s eta 0:00:01\n","   ----------------------------- ---------- 17.9/24.0 MB 7.0 MB/s eta 0:00:01\n","   ------------------------------ --------- 18.3/24.0 MB 7.5 MB/s eta 0:00:01\n","   ------------------------------- -------- 18.7/24.0 MB 7.5 MB/s eta 0:00:01\n","   ------------------------------- -------- 19.0/24.0 MB 7.5 MB/s eta 0:00:01\n","   -------------------------------- ------- 19.5/24.0 MB 7.6 MB/s eta 0:00:01\n","   --------------------------------- ------ 20.0/24.0 MB 7.8 MB/s eta 0:00:01\n","   --------------------------------- ------ 20.2/24.0 MB 7.7 MB/s eta 0:00:01\n","   ---------------------------------- ----- 20.7/24.0 MB 8.1 MB/s eta 0:00:01\n","   ----------------------------------- ---- 21.1/24.0 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------------ --- 21.6/24.0 MB 8.3 MB/s eta 0:00:01\n","   ------------------------------------ --- 21.9/24.0 MB 8.2 MB/s eta 0:00:01\n","   ------------------------------------- -- 22.3/24.0 MB 8.4 MB/s eta 0:00:01\n","   ------------------------------------- -- 22.8/24.0 MB 8.4 MB/s eta 0:00:01\n","   -------------------------------------- - 23.0/24.0 MB 8.2 MB/s eta 0:00:01\n","   ---------------------------------------  23.5/24.0 MB 8.5 MB/s eta 0:00:01\n","   ---------------------------------------  24.0/24.0 MB 8.5 MB/s eta 0:00:01\n","   ---------------------------------------  24.0/24.0 MB 8.5 MB/s eta 0:00:01\n","   ---------------------------------------  24.0/24.0 MB 8.5 MB/s eta 0:00:01\n","   ---------------------------------------- 24.0/24.0 MB 7.3 MB/s eta 0:00:00\n","Installing collected packages: gensim\n","Successfully installed gensim-4.3.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install BeautifulSoup4 scikit-learn textblob gensim\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics"]},{"cell_type":"markdown","metadata":{"id":"98GFAbEVrTRp"},"source":["Multinomial NB"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":1218,"status":"ok","timestamp":1701252178310,"user":{"displayName":"Monica Evelyn","userId":"07312078870992308835"},"user_tz":-420},"id":"NL9lDLZQh5ZA","outputId":"6bb47616-fa15-4f66-9d61-4ecd0cebfa43"},"outputs":[{"name":"stdout","output_type":"stream","text":["['happiness' 'happiness' 'happiness' ... 'happiness' 'love' 'happiness']\n","34211    happiness\n","27311      sadness\n","20969    happiness\n","3790     happiness\n","8443       sadness\n","           ...    \n","11379      sadness\n","5505       sadness\n","1637     happiness\n","9538       sadness\n","31103    happiness\n","Name: sentiment, Length: 1865, dtype: object\n","Accuracy: 0.5721179624664879\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# Load the training data from the CSV file\n","df_train_multinomial = df\n","\n","# Preprocess the training data\n","x = df_train_multinomial['cleaned_content']\n","y = df_train_multinomial['sentiment']\n","\n","# using count vectorizer\n","vectorizer = CountVectorizer()\n","x_train_vectorized = vectorizer.fit_transform(x)\n","# print(x_train_vectorized)\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(x_train_vectorized, y, test_size=0.12, random_state=42)\n","\n","# initialize multinomialNB classifier\n","classifier = MultinomialNB()\n","classifier.fit(X_train.toarray(), y_train)\n","\n","# Predict on the test data\n","y_pred = classifier.predict(X_test.toarray())\n","print(y_pred)\n","print(y_test)\n","# Evaluate the accuracy of the classifier\n","accuracy = metrics.accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.42503217503217505\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import svm\n","from sklearn import metrics\n","\n","# Assuming df_train_multinomial contains 'cleaned_content' and 'sentiment' columns\n","\n","# Preprocessing and tokenizing the text\n","tokenized_text = df_train_multinomial['content'].apply(lambda x: x.split())\n","\n","# Training the Word2Vec model\n","word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)\n","\n","# Creating word embeddings\n","def document_embedding(text):\n","    embedding = []\n","    for word in text:\n","        if word in word2vec_model.wv:\n","            embedding.append(word2vec_model.wv[word])\n","    return np.mean(embedding, axis=0) if embedding else np.zeros(100)  # Return zeros if no valid words found\n","\n","# Applying the Word2Vec embeddings to the text data\n","word_embeddings = np.vstack(tokenized_text.apply(document_embedding))\n","# print(word_embeddings)\n","# Scale the data to make it non-negative\n","scaler = MinMaxScaler()\n","word_embeddings_non_negative = scaler.fit_transform(word_embeddings)\n","\n","# Target labels\n","y = df_train_multinomial['sentiment']\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(word_embeddings_non_negative, y, test_size=0.2, random_state=42)\n","\n","# Initialize SVM classifier\n","classifier = svm.SVC(kernel='linear')  # You can specify different kernels: 'linear', 'rbf', 'poly', etc.\n","\n","# Fit the classifier to the training data\n","classifier.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred = classifier.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = metrics.accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"a_OGee20u1h5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.2384169884169884\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Load the training data from the CSV file\n","df_train_gaussian = df\n","\n","# Preprocess the training data\n","x = df_train_gaussian['cleaned_content']\n","y = df_train_gaussian['sentiment']\n","\n","# using count vectorizer\n","vectorizer = CountVectorizer()\n","x_train_vectorized = vectorizer.fit_transform(x)\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(x_train_vectorized, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Gaussian Naive Bayes classifier\n","gnb = GaussianNB()\n","\n","# Train the classifier on the training data\n","gnb.fit(X_train.toarray(), y_train)\n","\n","# Predict on the test data\n","y_pred = gnb.predict(X_test.toarray())\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = metrics.accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["COBA KNN BISMILAH"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.4687902187902188\n"]}],"source":["df_train_knn = df\n","\n","x = df_train_knn['cleaned_content']\n","y = df_train_knn['sentiment']\n","\n","# using count vectorizer\n","vectorizer = CountVectorizer()\n","x_train_vectorized = vectorizer.fit_transform(x)\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(x_train_vectorized, y, test_size=0.2)\n","\n","# Scale the features using StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train.toarray())\n","X_test = scaler.transform(X_test.toarray())\n","\n","# Initialize the KNearest Neighbors Classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","\n","y_pred = knn.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6185,"status":"ok","timestamp":1701251968097,"user":{"displayName":"Monica Evelyn","userId":"07312078870992308835"},"user_tz":-420},"id":"71YgmU6lnWpk","outputId":"9ccd0254-7187-4dfc-839e-5576af9921b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["python-requests/2.31.0\n","<Response [200]>\n","{'title': '', 'author': '', 'rating': '', 'categories': [], 'comments': [], 'sentiments': []}\n","<Response [200]>\n","{'title': '', 'author': '', 'rating': '', 'categories': [], 'comments': [], 'sentiments': []}\n","<Response [200]>\n","{'title': '', 'author': '', 'rating': '', 'categories': [], 'comments': [], 'sentiments': []}\n"]}],"source":["# Make a simple request to a website that echoes back user agent information\n","response = requests.get('https://httpbin.org/user-agent')\n","\n","# Extract and print your user agent string\n","user_agent = response.json()['user-agent']\n","print(user_agent)\n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","}\n","\n","titles = [\"magics-return-i-can-see-the-spirits\", \"taking-the-mafia-to-the-magic-world\", \"ill-surpass-the-mc\"]\n","\n","for title in titles:\n","    url = f'https://www.lightnovelworld.com/novel/{title}'\n","    response = requests.get(url, headers=headers)\n","    print(response)\n","    data_dict = {'title': '', 'author': '', 'rating': '', 'categories': [], 'comments': [], 'sentiments': []}\n","\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Get novel title\n","        title_element = soup.find('h1', class_='novel-title')\n","        if title_element:\n","            data_dict['title'] = title_element.text.strip()\n","\n","        # Get author information\n","        author_element = soup.find('span', attrs={'itemprop': 'author'})\n","        if author_element:\n","            data_dict['author'] = author_element.text.strip()\n","\n","        # Get rating information\n","        meta_rating_element = soup.find('meta', attrs={'itemprop': 'ratingValue'})\n","        if meta_rating_element:\n","            data_dict['rating'] = meta_rating_element.get('content', '').strip()\n","\n","        # Mencari semua elemen <p> di dalam <div> dengan class 'comment-text'\n","        comment_text_divs = soup.find_all('div', class_='comment-text')\n","\n","        for comment_text_div in comment_text_divs:\n","            p_elements = comment_text_div.find_all('p')\n","\n","            for p_element in p_elements:\n","                paragraph_text = p_element.text.strip()\n","                data_dict['comments'].append(paragraph_text)\n","\n","                # Perform sentiment analysis using TextBlob\n","                blob = TextBlob(paragraph_text)\n","                sentiment = 'positive' if blob.sentiment.polarity > 0 else 'negative' if blob.sentiment.polarity < 0 else 'neutral'\n","                data_dict['sentiments'].append(sentiment)\n","\n","        # Get Categories\n","        categories_header = soup.find('h4', string='Categories')\n","        if categories_header:\n","            categories_list = []\n","            ul_element = categories_header.find_next('ul')\n","            if ul_element:\n","                categories = ul_element.find_all('li')\n","                for category in categories:\n","                    category_text = category.text.strip()\n","                    categories_list.append(category_text)\n","                data_dict['categories'] = categories_list\n","\n","        # Print each comment with its predicted sentiment\n","        print(data_dict)\n","        for comment, sentiment in zip(data_dict['comments'], data_dict['sentiments']):\n","            print(f\"Comment: {comment}\")\n","            print(f\"Predicted Sentiment: {sentiment}\\n\")\n","    else:\n","        print(f\"Gagal mengambil halaman web untuk judul {title}. Kode status:\", response.status_code)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
